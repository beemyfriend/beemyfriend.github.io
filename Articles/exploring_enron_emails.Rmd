---
title: "Exploring Enron Emails"
author: "BeeMyFriend"
date: "March 31, 2017"
output:
  html_document:
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

The Enron Corpus is one of the largest dataset of emails available to the public. Normally, emails are a very personal and private thing, and  shouldn't be made available to the public. However, the Federal Energy Regulatory Commission acquired these emails during its investigation of the company in 2002 and placed the email corpus in the public domain once the investigation was over. The Enron Corpus contains over 500,000 emails generated by over 150 employees. It is a very large data set and is ideal for learning how to mine unstructured (text) data. 

In this article we will explore mining unstructured data by tidying the data into a data frame, analyzing the data using sentiment analysis, and also grouping the emails by using the unsupervised learning method known as the Latent Dirichlet Allocation Model (LDA).

The Enron Corpus can be found by going to [Carnegie Mellon University's School of Computer Science's website](https://www.cs.cmu.edu/~./enron/). Carnegie Mellon provides the corpus in it's raw format - that is, each email is given its own text file and is stored in a folder representating the original users' email folders. Luckily, William Cukierski was kind enough to provide the corpus in a CSV on [Kaggle's website](https://www.kaggle.com/wcukierski/enron-email-dataset). This article will use the CSV.

Finally, all analysis will be done on my local machine, which maxes out at about 2GB of RAM. My machine will not be able to compute an LDA model for a 500,000+ observation dataset and so I will gradually shrink the dataset down to a size that is manageable, but also will work towards shrinking the dataset down to a smaller dataset that makes sens - not simply removing random observations.

## Tidying the Data

First we will need to load the tidyverse. It is a wrapper for a number of useful packages including `dplyr`, `ggplot`, and `readr`.

```{r message = F}
library(tidyverse)
```

We will now load the data into R by using the `readr` function `read_csv`.

```{r eval = F}
enron_emails <- read_csv('emails.csv')
```

```{r echo = F, message = F}
enron_emails <- read_csv('D:/enron-email-dataset/enron_sent_box.csv')
library(stringr)
enron_emails$message <- str_replace_all(enron_emails$message, '\r', '')
set.seed(1234)
```

The csv provides only two variables: **file** and **message**. The **file** variable contains the original directory and filename of each email. The root level of this path is the employee to whom the email belongs.

```{r}
enron_emails$file %>%
  sample(5)
```

The **message** variable contains the email text. This includes email meta-data such as the *Message-ID*, *Date*, *To*, *Subject* fields as well as the body of the email. 

```{r}
enron_emails$message %>%
  sample(1)
```

First, to tackle this large dataset we should first create a smaller, more manageble subset. After looking at the different files, I decided that a decent subset would be emails found in the users' *sent_mail* folder. This seems like a good choice because it will only contain emails from Enron employees. The `stringr` package provides us with `str_detect()` which is a function that looks to see if a string can be found within another string. We can combine `str_detect()` with `dplyr`'s `filter()` to provide us all the observations which have *sent_mail* in their file-path.

```{r message = F}
library(stringr)

enron_emails <- enron_emails  %>%
  filter(str_detect(file, '/_sent_mail/'))
```

There are now only `r enron_emails %>% nrow()` observations to work with.

Next we need to split the the `message` variable into the new `header` and `body` variables. Luckily, the two are neatly seperated by two consecutive newlines. Before the two newlines is the header, and after the two newlines is the body. We should also clean the body a bit by removing all subsequent newlines and tabs, as well as well as removing all referenced emails. A referenced email is an email that was forwarded or copied from a previous email. Also, we will replace all email addresses with with an *EMAIL_ADRESS* token so that different emails don't have a strong influene on the analysis.

```{r}
enron_emails <- enron_emails %>%
  mutate(message = message %>% str_replace_all('\r', '')) %>%
  mutate(header = str_sub(message, end = str_locate(message, '\n\n')[,1] -1)) %>%
  mutate(body = str_sub(message, start = str_locate(message, '\n\n')[,2] + 1) %>% 
           str_replace_all('\n|\t', ' ') %>% 
           str_replace_all('---Original Message .*', 'FORWARDED_MESSAGE') %>% 
           str_replace_all('--- Forwarded by .*', 'FORWARDED_MESSAGE') %>%
           str_replace_all('From: .*', 'FORWARDED_MESSAGE') %>%
           str_replace('To:.*', 'FORWARDED_MESSAGE') %>%
           str_replace_all('\\S*@\\S*', 'EMAIL_ADDRESS')) 
```

Now we will split the header into its seperate metadata fields. 

```{r}
enron_emails <- enron_emails %>%
  mutate(date = str_extract(header, 'Date:.*') %>% 
           str_replace('Date: ', '') %>% 
           str_replace('.+, ', '') %>% 
           strptime(format = '%d %b %Y %H:%M:%S %z') %>%
           as.POSIXct()) %>%
  mutate(from = str_extract(header, 'From:.*') %>% 
           str_replace('From: ', '')) %>%
  mutate(to = header %>% str_replace_all('\n|\t', ' ') %>%
           str_extract('To:.*Subject:') %>%
           str_replace_all('To: |Subject:', '')) %>%
  mutate(subject = str_extract(header, 'Subject:.*') %>% 
           str_replace('Subject: ', '')) %>%
  mutate(xfrom = str_extract(header, 'X-From:.*') %>% 
           str_replace('X-From: ', '')) %>%
  mutate(xto = str_extract(header, 'X-To:.*') %>% 
           str_replace('X-To: ', '')) %>%
  mutate(xcc = str_extract(header, 'X-cc:.*') %>% 
           str_replace('X-cc: ', '')) %>%
  mutate(xbcc = str_extract(message, 'X-bcc:.*') %>% 
           str_replace('X-bcc: ', '')) %>%
  arrange(date)

```

##Sentiments: 

In order to analyse the sentiments of each email we will need to split the body of each email into its individual words. We will be using the **AFINN** sentiment library to score each word with either a positive number or a negative number depending on the individual word's sentiment.

```{r message = F}
library(tidytext)
library(tm)
```

In order to analyze the sentiments of the body of an email, we will need to create a matrix of each word within the body. The matrix shows provides the number of times a word shows up in an email. We can create such a matrix for every body of email by using the `tm` package's `DocumentTermMatrix()` function. A document term matrix is a matrix that provides a word count for each document provided. For us, a document is a single body of an email. Before we can create the document term matrix, however, we will need to convert the vector of email bodies into a corpus by using the `Corpus()` function.  A corpus is a data structure for a collection of text and it is necessary to convert a vector of characters into a corpus in order to perform many different text-mining tasks.

We will need to provide a control for making the document term matrix that accounts for punctuation, numbers, and stopwords. A stop word is a word that shows up so frequently in the english language that it doesn't have any analytical value. For example 'the' and 'a' are neither positive words, nor negative words, and are also used in almost every sentence so they can't be used to differentiate how different people speak.

```{r}

dtm.control = list(
  removePunctuation = T,
  removeNumber = T,
  stopwords = stopwords('english')
)

enron_dtm <- Corpus(VectorSource(enron_emails$body)) %>%
  DocumentTermMatrix(dtm.control)
```

Once the document term matrix is created, we will need to convert the matrix into a tidy data frame using `tidytext`'s `tidy()` function. The tidy data frame will have three variables - **document**, **term**, and **count**. The document variable is an email body, where a document of 1 is the first email body in our `enron_emails` data frame. The term variable contains each word found in each document. Each document-word combination is given its own row.The count variable is the number of times a particular term (or word) is found in a document. 

```{r}
enron_sentiments <- tidy(enron_dtm)

enron_sentiments %>% head()
```

Once we create the tidy dataframe we will join it with the 'AFINN' library and create the **score** variable by multiplying the word's sentiment score with the the number of times a word appears in a document. We can then sum the score of each word within each document to create a **sentiment** variable for each document.

```{r}
enron_sentiments <- enron_sentiments %>%
  inner_join(get_sentiments('afinn'), by = c(term = 'word')) %>%
  mutate(score = score * count) %>%
  group_by(document) %>%
  nest() %>%
  mutate(sentiment = sapply(seq_along(.$data), function(i){
    .$data[[i]]['score'] %>%
      sum()
  })) %>%
  select(-data)

enron_sentiments %>% head()
```

The document term matrix numbers the documents according to order of appearence in the dataframe, so in order to combine the information with our original `enron_emails` data frame we will need to create a new variable called *document* and order it from 1 to `nrow(enron_emails)`

We will also clean the date variable so we can analyze the sentiments for each day of the week and each month. We can do this by using the `lubridate`'s `wday()` and `month()` functions.

```{r message = F}
library(lubridate)

enron_emails <- enron_emails %>%
  mutate(weekday = wday(date, label = T)) %>%
  mutate(month = month(date, label = T)) %>%
  mutate(document = 1:nrow(.) %>% as.character()) %>%
  left_join(enron_sentiments) 
```
Emails that don't have any sentiment are given `NA` values. We will need to convert these values into `0`.

```{r}
enron_emails$sentiment[is.na(enron_emails$sentiment)] = 0
```

Some of the emails were sent to multiple different people. We should split the `to` variable so that each reciepient of an email is accounted for. Then we can find the averge email sentiment over the course of all the correspondence between two people. 

```{r}
enron_emails <- enron_emails %>%
  mutate(to = str_split(to, ',')) %>%
  unnest() %>%
  group_by(from, to) %>%
  nest() %>%
  mutate(mean_sentiment = sapply(seq_along(.$data), function(i){.$data[[i]]$sentiment %>% mean()})) %>%
  mutate(correspondence = sapply(seq_along(.$data), function(i){.$data[[i]] %>% nrow()})) %>%
  arrange(desc(correspondence))

```

## Visualizations

Let's check the check the mean sentiment for each correspondence. Since their are a lot of email recipients, we will need to shrink the number of 'to' fields. I think the best way to do this is by focusing only on email correspondence that contain more than 10 emails. This let's us focus on people who write to eachother somewhat frequently. 

```{r}
ggplot(enron_emails %>% filter(correspondence > 10), aes(to, from)) +
  geom_point(aes(color = mean_sentiment)) +
  scale_color_gradient2(low = 'brown',
                        mid = 'yellow',
                        high = 'blue') +
  theme(axis.text.x = element_blank(), 
        axis.ticks.x = element_blank())

```

We don't have too much variation in with the sentiments. This shouldn't be too surprising since we're analyzing work emails. 

We can also see whether an employee wrote a lot of emails to a single person, or multiple people, or both.

```{r}

ggplot(enron_emails %>% filter(correspondence > 10), aes(to, from)) +
  geom_point(aes(size = correspondence)) +
  theme(axis.text.x = element_blank(),
        axis.ticks.x = element_blank())
```

Vince Kaminski seems to be sending a lot of emails to one particular person. While Kay Mann, Eric Bass, and Chris Germany seem to be sending multiple emails to many different people. 

Let's look into this further...

```{r}
enron_emails %>%
  unnest() %>%
  group_by(from, to, correspondence) %>%
  nest() %>%
  arrange(desc(correspondence)) %>%
  select(-data)
```

It turns out that Vince is forwarding many of his work emails to his personal email account. We should probably get rid of these email correspondence because they don't actually represent email communication between two different people. 

Also, to improve visualization and also provide a more concise dataset to model, we should only keep the top ten most prolific email writers.

```{r}
top_ten_writers <- enron_emails %>% 
  unnest()  %>%
  filter(!(str_detect(from, 'vince.kaminski') & str_detect(to, 'vkaminski'))) %>%
  group_by(from, body) %>%
  nest() %>%
  group_by(from) %>%
  nest() %>%
  mutate(tot_emails_sent = sapply(seq_along(.$data), function(i){
      nrow(.$data[[i]])
    })) %>%
  arrange(desc(tot_emails_sent)) %>%
  .[1:10,] %>%
  unnest() 
```

Let's visualize both the sentiment and correspondence once more, but with just the top ten email writers.

```{r}
ggplot(top_ten_writers %>% unnest(), aes(to, from)) +
  geom_point(aes(color = mean_sentiment)) +
  scale_color_gradient2(low = 'brown',
                        mid = 'yellow',
                        high = 'blue') +
  theme(axis.text.x = element_blank(), axis.ticks = element_blank())

```

I will need to check my code on this one. The `mean_sentiment` scale changed drastically... If everything is correct, then we learn that Susan Scott is a more positive writer in general and Chris Germany is really positive with one other person.

```{r}
top_ten_writers %>%
  unnest() %>% 
  arrange(desc(sentiment)) %>%
  .$body %>%
  .[1]
```

It turns out the most positive email was a compilation of 5 posting to producers in West Virginia. While the overall tone of these postings is cordial, I wouldn't say that they are particularly positive. This is an instance where the more words on a document can skew its sentiment score. 


```{r}
top_ten_writers %>%
  unnest() %>% 
  arrange(desc(sentiment)) %>%
  .$body %>%
  .[5]
```

The second most positive email was recounting a school dance competition where the writer's daughter's team won first place. Unlike the previous email, this was not a professional email, the body of the email was relatively short, and the tone of the email was incredibly positive. 

Let's move on to the top-ten writers correspondence chart now that we removed Vince's emails to himself.

```{r}
ggplot(top_ten_writers %>% unnest(), aes(to, from)) +
  geom_point(aes(size = correspondence)) +
  theme(axis.text.x = element_blank(), axis.ticks = element_blank())
```

It turns out Kay Mann write the most emails in general and also seems to write the most emails to individuals as well. Let's verify this by checking all correspondence greater than 100.

```{r}
top_ten_writers %>% 
  unnest() %>% 
  filter(correspondence > 100) %>% 
  group_by(from, to, correspondence) %>% 
  nest() %>%
  arrange(desc(correspondence)) %>%
  select(-data)
```


```{r}
ggplot(top_ten_writers %>% unnest(), aes(month, sentiment)) +
  geom_boxplot()

ggplot(top_ten_writers %>% unnest(), aes(weekday, sentiment)) +
  geom_boxplot()

```

## Machine Learning: Latent Dirichlet Allocation

We will use the `top_ten_writers` dataframe for modelling. It is a subset of the original Enron Corpus, but with only `r top_ten_writers %>% nrow()` observations. The model we will be using is called the **Latent Dirichlet Allocation (LDA)** model. LDA creates topics by analyzing the co-occurances of words within different documents. If two words are frequently present in the same documents, then they are more likely to be part of the same topic. If two words are almost never present in the same documents at once, then they are more likely to be part of different topics. It is important to note that a word can appear in multiple topics. LDA belongs to the Bayesian family of models and you can learn more about the statistics involved by going to [Journal of Machine Learning Research](http://jmlr.csail.mit.edu/papers/v3/blei03a.html).

The `topicmodels` package provides the function `LDA()` for Latent Dirichlet Allocation (not to be confused by a similiar R function `lda()` used for Linear Discriminant Analysis). `LDA()`requires a document term matrix as an input. When creating the document term matrix, we will need to remove sparse terms, that is, we will need to remove words that rarely show up in the Enron Corpus. Removing infrequent words is necessary in order to use the `LDA()` function.

```{r warning = F}
dtm.control = list(
  tolower = T,
  removePunctuation = T,
  removeNumbers = T,
  stopwords = stopwords('english'),
  weighting = weightTf,
  seed = 0
)

enron_dtm <- Corpus(VectorSource(top_ten_writers$body)) %>%
  DocumentTermMatrix( control = dtm.control) %>%
  removeSparseTerms(.999) %>%
  .[rowSums(as.matrix(.))>0,]
```

Once the document term matrix is created, we can make the model with `LDA()`. We will arbitrarily choose to create 4 different topics. Once the model is created, then we can see the most frequently used words in each topic by using the `terms()` function. 

```{r message = F}
library(topicmodels)
enron_body_lda <- LDA(enron_dtm, k = 4)
terms(enron_body_lda, 20)

```

## Networks

It would be interesting to see the relationship between different employees within Enron. The best way to do this is by filtering for only Enron employees within the *to* variable. Better yet, let's filter for email recipients who have a sent email on record in the Enron Corpus. This guarentees that there is at least a one way connection. 

```{r warning = F, message=F}
enron_network <- enron_emails %>%
  unnest() %>%
  group_by(from, to, correspondence) %>%
  nest() %>%
  filter(to %in% from) %>%
  select(-data) %>%
  mutate(correspondence = sapply(correspondence, function(x){
    if(x < 9){
        'Few'
      } else if ( x < 17) {
        'Medium'
      } else {
        'Many'
      }}) %>%
      as.factor()) 
```

`ggraph`is a new `ggplot` extension that is all the rage. It can make pretty neat network graphs pretty easily. However, proper use of `ggraph` depends on the user being literate in the `igraph` package. It is absolutely worth checking out Katherine Ognyanova's `igraph` tutorial ['Network Analysis and Visualization with R and igraph'](http://kateto.net/networks-r-igraph). 

```{r warning = F, message = F}

library(ggraph)
library(igraph)

set.seed(4321)
graph <- graph_from_data_frame(enron_network) 

ggraph(graph, layout = 'kk') +
  geom_edge_fan(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_edge_loop(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_node_point() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank())

ggraph(graph, layout = 'fr') +
  geom_edge_fan(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_edge_loop(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_node_point() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank())

ggraph(graph, layout = 'linear') +
  geom_edge_fan(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_edge_loop(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_node_point() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank())

ggraph(graph, layout = 'linear', circular = T) +
  geom_edge_fan(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_edge_loop(aes(color = correspondence), width = 1, arrow = arrow(length = unit(4, 'mm')), start_cap = circle(3, 'mm'), end_cap = circle(3, 'mm')) +
  geom_node_point() +
  theme(axis.text = element_blank(), axis.ticks = element_blank(), axis.title = element_blank())
```



## Conclusion